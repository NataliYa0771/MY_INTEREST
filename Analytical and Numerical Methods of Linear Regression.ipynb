{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аналитические и численные методы линейной регрессии\n",
    "\n",
    "###  В этом ноутбуке рассматриваются различные подходы к решению задачи **линейной регрессии** с использованием **L2-потери** (квадратичной ошибки) $$ \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 $$\n",
    "где:\n",
    "- $\\mathbf{X}$ — матрица признаков,\n",
    "- $\\mathbf{w}$ — вектор весов (параметров модели),\n",
    "- $\\mathbf{y}$ — вектор целевой переменной.\n",
    "\n",
    "Эта постановка известна как **метод наименьших квадратов (OLS)**. Одно из её ключевых преимуществ — существование **АНАЛИТИЧЕСКОГО решения в замкнутой форме**:\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "при условии, что матрица $\\mathbf{X}^T\\mathbf{X}$ обратима. Это позволяет находить оптимальные веса без итераций — как в случае одного признака, так и нескольких.\n",
    "\n",
    "Мы сравниваем:\n",
    "- Аналитические (непараметрические) решения вручную,\n",
    "- Реализацию `LinearRegression` из `scikit-learn`, которая использует SVD-разложение для численно устойчивого решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему мы не рассматриваем минимизацию $ \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1 $?\n",
    "\n",
    "В этом ноутбуке мы фокусируемся на задаче линейной регрессии с **L2-потерей** (метод наименьших квадратов),\n",
    "для которой существуют **аналитические решения** — как в случае одного признака, так и в многомерном случае. \n",
    "Однако, если вместо L2 использовать **L1-норму**:\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1,\n",
    "$$\n",
    "— **аналитического решения в замкнутой форме не существует**. Причина в том, что L1-норма не является гладкой функцией (её производная не определена при нулевых остатках), и стандартные методы математического анализа для поиска минимума через приравнивание градиента к нулю неприменимы.\n",
    "\n",
    "- **Задача минимизации L1-потери** (т.е. $\\min \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1$) на практике решается следующими способами:\n",
    "\n",
    "  1. **\"Из коробки\"**:\n",
    "     - `QuantileRegressor(quantile=0.5)` — в `scikit-learn`, эквивалентна L1-регрессии.\n",
    "     - `LADRegressor` — начиная с `scikit-learn >= 1.3`, прямая реализация.\n",
    "     Оба метода используют **линейное программирование** под капотом (например, солвер `highs`).\n",
    "     Как L1-регрессия сводится к задаче линейного программирования (LP)?\n",
    "\n",
    "     Задача минимизации L1-потери $ \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1 $ не имеет аналитического              решения, но может быть **точно решена численно**, так как её можно **переформулировать как задачу линейного          программирования (Linear Programming, LP)**. Переформулировка задачи:\n",
    "\n",
    "     Вводятся вспомогательные переменные $\\mathbf{r}^+ \\in \\mathbb{R}^n$, $\\mathbf{r}^- \\in \\mathbb{R}^n$ — положительные и отрицательные части остатков.\n",
    "\n",
    "    Каждый остаток $x_i^T \\mathbf{w} - y_i$ можно представить как $ x_i^T \\mathbf{w} - y_i = r_i^+ - r_i^-, \\quad \\text{где } r_i^+ \\geq 0, \\; r_i^- \\geq 0 $. Тогда модуль остатка $|x_i^T \\mathbf{w} - y_i| = r_i^+ + r_i^- $\n",
    "\n",
    "    Формальная постановка LP-задачи $\\min_{\\mathbf{w},\\, \\mathbf{r}^+,\\, \\mathbf{r}^-} \\sum_{i=1}^n (r_i^+ + r_i^-)$ при ограничениях $\\mathbf{X}\\mathbf{w} - \\mathbf{y} = \\mathbf{r}^+ - \\mathbf{r}^-$,  $\\mathbf{r}^+ \\geq 0^-, \\quad \\mathbf{r}^- \\geq 0 $\n",
    "\n",
    "    Целевая функция и ограничения линейные, это классическая задача **линейного программирования**.\n",
    "    \n",
    "    \n",
    "  2. **Через `scipy.optimize.linprog`**:\n",
    "     - Можно сформулировать задачу как LP вручную.\n",
    "     - Точно, но требует ручного построения матриц ограничений.\n",
    "     - Подходит для малых задач.\n",
    "\n",
    "  3. **Субградиентный спуск вручную**:\n",
    "     - Реализуется с использованием `np.sign(остатки)`.\n",
    "     - Просто, но медленно и требует подбора шага.\n",
    "     - Нет в `SGDRegressor` — нужно писать самому.\n",
    "\n",
    "- **Добавление L1 или L2-регуляризации к L1-потере**:\n",
    "  - Теоретически возможно:  \n",
    "    $\n",
    "    \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1 + \\alpha_1 \\|\\mathbf{w}\\|_1 + \\alpha_2 \\|\\mathbf{w}\\|_2^2\n",
    "    $\n",
    "  - **Но на практике**:\n",
    "    - Не реализовано в `scikit-learn`,\n",
    "    - Не используется массово,\n",
    "    - Реализуется **вручную** через библиотеки вроде `cvxpy`.\n",
    " \n",
    "> Итак, для L1-потери **лучше использовать `QuantileRegressor` или `LADRegressor` из коробки**.  \n",
    "> Всё остальное — либо ручная реализация, либо специализированные инструменты вроде `cvxpy`, которые нужны только в особых случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет (один признак):\n",
      "    x     y\n",
      "0   1   3.5\n",
      "1   2   6.8\n",
      "2   3   9.2\n",
      "3   4  12.0\n",
      "4   5  15.1\n",
      "5   6  18.3\n",
      "6   7  21.0\n",
      "7   8  24.2\n",
      "8   9  27.1\n",
      "9  10  30.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Датасет: 10 наблюдений, один признак\n",
    "data_single = {\n",
    "    'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'y': [3.5, 6.8, 9.2, 12.0, 15.1, 18.3, 21.0, 24.2, 27.1, 30.5]\n",
    "}\n",
    "\n",
    "df_single = pd.DataFrame(data_single)\n",
    "print(\"Датасет (один признак):\")\n",
    "print(df_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Аналитическое решение парной линейной регрессии\n",
    "\n",
    "Для случая **ОДНОГО** признака $ x $ и целевой переменной $ y $ можно найти оптимальные параметры модели $ y = mx + b $ **аналитически**, без итераций.\n",
    "\n",
    "###  Цель\n",
    "Найти такие $ m $ (наклон) и $ b $ (свободный член), которые минимизируют сумму квадратов ошибок (MSE).\n",
    "\n",
    "---\n",
    "\n",
    "###  Формулы МНК (метод наименьших квадратов)\n",
    "\n",
    "Для $ n $ наблюдений:\n",
    "\n",
    "- **Наклон $ m $:**\n",
    "  $$\n",
    "  m = \\frac{n \\sum (x_i y_i) - \\sum x_i \\sum y_i}{n \\sum (x_i^2) - (\\sum x_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Свободный член $ b $:**\n",
    "  $$\n",
    "  b = \\bar{y} - m \\bar{x}\n",
    "  $$\n",
    "  где $ \\bar{x} $ и $ \\bar{y} $ — средние значения.\n",
    "\n",
    "Эта формула — частный случай метода наименьших квадратов (МНК), выведенный только для случая одного признака x .\n",
    "Она не работает при двух и более признаках, потому что: \n",
    "\n",
    "- теряется возможность учитывать взаимное влияние признаков,\n",
    "- нельзя выразить несколько коэффициентов в одной скалярной формуле.\n",
    "\n",
    "\n",
    "###  Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 1 (ручная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "points = list(df_single.itertuples(index=False))\n",
    "\n",
    "n = len(points)\n",
    "\n",
    "# Вычисляем m (наклон)\n",
    "m = (n * sum(p.x * p.y for p in points) - sum(p.x for p in points) * sum(p.y for p in points)) / \\\n",
    "    (n * sum(p.x**2 for p in points) - (sum(p.x for p in points))**2)\n",
    "\n",
    "# Вычисляем b (свободный член)\n",
    "b = (sum(p.y for p in points) / n) - m * (sum(p.x for p in points) / n)\n",
    "\n",
    "print(f\"\\nМетод 1 (ручная формула):\")\n",
    "print(f\"y = {m:.3f}x + {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Аналитическое (матричное) решение парной и множественной линейной регрессии (МНК)\n",
    "\n",
    "Этот метод позволяет найти оптимальные параметры модели \\( y = mx + b \\) аналитически, используя линейную алгебру. Он работает не только для одного признака, но и для любого числа признаков — это универсальный подход, основанный на методе наименьших квадратов (МНК).\n",
    "\n",
    "### Математическая формула\n",
    "\n",
    "Оптимальные параметры $ \\mathbf{w} $ находятся по формуле метода наименьших квадратов:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $ X $ — матрица признаков с добавленным столбцом единиц (для свободного члена $ b $)\n",
    "- $ \\mathbf{y} $ — вектор целевых значений\n",
    "- $ \\mathbf{w} = [b, m] $ — вектор параметров модели\n",
    "\n",
    "\n",
    "Хотя в теории решение выражается формулой, на практике в библиотеках, включая `scikit-learn`, она **не используется напрямую** из-за численной неустойчивости и может приводить к значительным ошибкам в присутствии мультиколлинеарности, шума или при большом количестве признаков.\n",
    "\n",
    "### Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 2 (матричная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(n), df_single['x']])  # [1, x]\n",
    "y = df_single['y'].values\n",
    "\n",
    "# Аналитическое решение: w = (X^T X)^{-1} X^T y\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "b_mnk, m_mnk = w[0], w[1]\n",
    "\n",
    "print(f\"\\nМетод 2 (матричная формула):\")\n",
    "print(f\"y = {m_mnk:.3f}x + {b_mnk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Метод `LinearRegression` из `scikit-learn`\n",
    "\n",
    "Внутри `LinearRegression` используется функция `scipy.linalg.lstsq` (или аналог из `numpy`), которая решает задачу наименьших квадратов **с помощью SVD** — сингулярного разложения.\n",
    "\n",
    "Матрица признаков $\\mathbf{X}$ разлагается как:\n",
    "$\n",
    "\\mathbf{X} = U \\Sigma V^T\n",
    "$\n",
    "где:\n",
    "- $U$ и $V$ — ортогональные матрицы,\n",
    "- $\\Sigma$ — диагональная матрица сингулярных значений.\n",
    "\n",
    "Реальные данные почти всегда содержат шум, мультиколлинеарность, избыток признаков, пропущенные или коррелирующие переменные, что делает матрицу X плохо обусловленной или даже вырожденной, поэтому под капотом вызывается SVD с обрезанием малых сингулярных значений, и решение вычисляется через псевдообратную матрицу Мура-Пенроуза\n",
    "\n",
    "Вектор весов вычисляется как:\n",
    "$\n",
    "\\mathbf{w} = \\mathbf{X}^+ \\mathbf{y} = V \\Sigma^+ U^T \\mathbf{y}\n",
    "$\n",
    "где $\\mathbf{X}^+$ — **матрица Мура – Пенроуза** (псевдообратная матрица), а $\\Sigma^+$ — обобщённая обратная к $\\Sigma$:\n",
    "- $(\\Sigma^+)_{ii} = 1 / \\sigma_i$, если $\\sigma_i > 0$,\n",
    "- $(\\Sigma^+)_{ii} = 0$, если $\\sigma_i = 0$.\n",
    "\n",
    "> Это решение:\n",
    "> - Всегда существует,\n",
    "> - Имеет минимальную норму среди всех возможных решений,\n",
    "> - Устойчиво к мультиколлинеарности и вырожденности,\n",
    "> - Работает даже при $p > n$ (больше признаков, чем объектов).\n",
    "\n",
    "Существует еще и частный случай $\\mathbf{w} = V \\Sigma^{-1} U^T \\mathbf{y}$ (с обычным $\\Sigma^{-1}$), но работает только при полном ранге $\\mathbf{X}$, а **на практике используется обычно обобщённая версия с $\\Sigma^+$** — она универсальна и численно устойчива.\n",
    "\n",
    "---\n",
    "\n",
    "### Почему не используется формула $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$?\n",
    "\n",
    "Несмотря на математическую привлекательность аналитической формулы, её прямое применение в реальных данных проблематично:\n",
    "\n",
    "| Проблема                   | Описание |\n",
    "|----------------------------|--------|\n",
    "| **Численная неустойчивость** | При мультиколлинеарности $\\mathbf{X}^T\\mathbf{X}$ становится плохо обусловленной, и обращение приводит к большим ошибкам. |\n",
    "| **Потеря точности**         | Вычисление $\\mathbf{X}^T\\mathbf{X}$ удваивает ошибку округления. |\n",
    "| **Вырожденность**           | Если $p > n$ или признаки линейно зависимы, $\\mathbf{X}^T\\mathbf{X}$ необратима. |\n",
    "| **Риск переполнения**       | При больших значениях в $\\mathbf{X}$ произведение $\\mathbf{X}^T\\mathbf{X}$ может привести к переполнению. |\n",
    "\n",
    "---\n",
    "\n",
    "### Преимущества SVD в `LinearRegression`\n",
    "\n",
    "- Высокая численная устойчивость,\n",
    "- Работа с вырожденными и плохо обусловленными матрицами,\n",
    "- Поддержка $p > n$,\n",
    "- Контроль устойчивости через параметр `rcond` (порог для малых сингулярных значений).\n",
    "\n",
    "Использование SVD и псевдообратной матрицы делает `LinearRegression` из `scikit-learn` **надёжным и универсальным** даже на \"грязных\" или сложных по структуре данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы решения задачи линейной регрессии в `scikit-learn`\n",
    "\n",
    "В данном исследовании мы используем **SVD-решение** (`LinearRegression`), но в `scikit-learn` также доступны другие аналитические методы для решения задачи:\n",
    "\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_2^2\n",
    "$\n",
    "\n",
    "#### 1. Основные подходы\n",
    "\n",
    "- **`LinearRegression`**  \n",
    "  - **Алгоритм**: SVD или QR-разложение  \n",
    "  - **Формула решения**:  \n",
    "   $ \n",
    "    \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "    \\$ \n",
    "    \n",
    "  - **Особенности**:  \n",
    "    - Нет регуляризации ($\\alpha = 0$)  \n",
    "    - Устойчив к вырожденным или плохо обусловленным матрицам за счёт SVD  \n",
    "    - Работает даже при линейной зависимости признаков  \n",
    "\n",
    "- **`Ridge` с `solver='svd'`**  \n",
    "  - **Формула**:  \n",
    "    $\n",
    "    \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "    $ \n",
    "  - **Отличие**: добавлена **L2-регуляризация** (параметр `alpha > 0`)  \n",
    "  - Улучшает устойчивость при мультиколлинеарности  \n",
    "\n",
    "#### 2. Альтернативные аналитические методы\n",
    "\n",
    "- **`Ridge` с `solver='cholesky'`**  \n",
    "  - Использует разложение Холецкого для $\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I}$  \n",
    "  - **Плюсы**: высокая скорость при малом числе признаков  \n",
    "  - **Минусы**: требует положительной определённости матрицы (регуляризация обязательна)  \n",
    "\n",
    "- **`Ridge` с `solver='lsqr'`**  \n",
    "  - Использует итерационный метод на основе QR-разложения  \n",
    "  - Поддерживает разреженные матрицы  \n",
    "  - Эффективен при большом числе признаков или наблюдений  \n",
    "\n",
    "#### 3. Сравнительная таблица методов\n",
    "\n",
    "| Метод                     | Алгоритм          | Регуляризация | Устойчивость       | Скорость     |\n",
    "|---------------------------|-------------------|---------------|--------------------|--------------|\n",
    "| `LinearRegression`        | SVD / QR          | Нет           | Высокая            | Средняя      |\n",
    "| `Ridge(solver='svd')`     | SVD               | L2            | Очень высокая      | Медленная    |\n",
    "| `Ridge(solver='cholesky')`| Холецкого         | L2            | Средняя (с $\\alpha$)| Быстрая      |\n",
    "| `Ridge(solver='lsqr')`    | Итеративный QR    | L2            | Высокая            | Очень быстрая|\n",
    "\n",
    "> Все перечисленные методы:\n",
    "> - Являются **аналитическими** (или основаны на прямых матричных разложениях)  \n",
    "> - Решают обобщённую задачу минимизации:  \n",
    ">   $\n",
    "   \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_2^2\n",
    "   $\n",
    "> - **Не требуют подбора шага обучения** или числа итераций  \n",
    "> - Отличаются по устойчивости, скорости и требованиям к данным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неаналитические методы для задачи L2-потери\n",
    "\n",
    "Хотя для задачи минимизации L2-потери  \n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2\n",
    "$\n",
    "существуют аналитические решения (например, через SVD), при больших объёмах данных они становятся вычислительно затратными.  \n",
    "В таких случаях используются **итерационные (неаналитические) методы**, не требующие обращения матриц: Gradient Descent, Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, SAG, SAGA, LSQR, Coordinate Descent, ADMM.\n",
    "\n",
    "##  Методы решения задачи линейной регрессии с L2-регуляризацией: аналитические и численные подходы\n",
    "\n",
    "\n",
    "| Метод | Алгоритм | Регуляризация | Подходит для больших данных | Библиотеки и реализация | Особенности | Когда использовать |\n",
    "|-------|---------|---------------|-------------------------------|--------------------------|-------------|----------------------|\n",
    "| LinearRegression | SVD или QR-разложение | Нет | Нет (при \\( n, p > 10^4 \\) медленно) | `scikit-learn`: `LinearRegression` | Точный, устойчивый, но не масштабируемый | Когда данные небольшие, нет переобучения, и нужна максимальная точность без регуляризации |\n",
    "| Ridge (solver='svd') | SVD разложение | L2 | Нет | `scikit-learn`: `Ridge(solver='svd')` | Точный, работает при коллинеарности, медленно на больших данных | Для малых данных с мультиколлинеарностью, когда важна точность и интерпретируемость |\n",
    "| Ridge (solver='cholesky') | Разложение Холецкого | L2 | Средне | `scikit-learn`: `Ridge(solver='cholesky')` | Быстро при малом числе признаков, требует \\( \\alpha > 0 \\) | Когда \\( n, p < 10^4 \\), данные плотные, нужна быстрая сходимость |\n",
    "| Ridge (solver='lsqr') | Итерационный QR | L2 | Да | `scikit-learn`: `Ridge(solver='lsqr')` | Поддерживает разреженные матрицы, средняя скорость, рекомендуется для больших данных | Универсальный выбор для средних и больших данных, особенно с разреженными признаками |\n",
    "| Ridge (solver='sag') | Стохастический усреднённый градиент (SAG) | L2 | Да (при \\( n, p > 10^4 \\)) | `scikit-learn`: `Ridge(solver='sag')`; альтернативы: `river` | Быстрая сходимость, требует масштабирования признаков | Для больших, но умеренных по размеру выборок (\\( n > 10^4 \\)), если признаки стандартизованы |\n",
    "| Ridge (solver='saga') | Улучшенный SAG (SAGA) | L2 | Да | `scikit-learn`: `Ridge(solver='saga')`; альтернативы: `river`, `pytorch-optimizer` | Поддерживает L2; более устойчив, чем SAG, подходит для больших выборок | Аналог SAG, но лучше ведёт себя при сложных данных; хорош для больших и разреженных данных |\n",
    "| SGDRegressor | Стохастический градиентный спуск (SGD) | L2 (опционально) | Да (очень большие выборки) | `scikit-learn`: `SGDRegressor(loss='squared_error', penalty='l2')`; альтернативы: `river`, `PyTorch`, `TensorFlow` | Высокая гибкость, требует подбора шага обучения и масштабирования признаков, подходит для онлайн-обучения | Когда данные очень большие (не помещаются в память), потоковые, или нужно онлайн-обучение |\n",
    "\n",
    "\n",
    "Перечисленные методы охватывают **разные подходы к решению задачи линейной регрессии с L2-регуляризацией**, и их выбор зависит от размера данных, требований к скорости, точности и масштабируемости.\n",
    "\n",
    "- **Аналитические методы** (`LinearRegression`, `Ridge` с `svd`, `cholesky`, `lsqr`) дают точное решение и **не требуют масштабирования признаков**, но **не масштабируются** на очень большие выборки.\n",
    "- **Градиентные методы** (`Ridge` с `sag`, `saga`, `SGDRegressor`) являются **итерационными**, **масштабируемыми** и подходят для больших данных, но **требуют стандартизации признаков** и тщательного подбора гиперпараметров.\n",
    "- `SGDRegressor` — **наиболее гибкий** метод: поддерживает онлайн-обучение, потоковые данные и различные типы регуляризации, но требует настройки шага обучения.\n",
    "- Для **разреженных данных** рекомендуются `Ridge(solver='lsqr')`, `'sag'`, `'saga'` или `SGDRegressor`.\n",
    "- При **малом числе наблюдений** и необходимости точного решения лучше использовать `LinearRegression` или `Ridge` с `svd`/`cholesky`.\n",
    "\n",
    "Таким образом, выбор метода должен основываться на балансе между **точностью**, **скоростью**, **объёмом данных** и **практическими ограничениями** (например, память, онлайн-обучение).\n",
    "\n",
    "---\n",
    "\n",
    "### L1-регуляризация (Lasso)\n",
    "\n",
    "L1-регуляризация добавляется к L2-потере для получения разреженных решений:\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_1\n",
    "$\n",
    "\n",
    "Эта задача известна как **Lasso-регрессия**.  \n",
    "Она не имеет аналитического решения из-за недифференцируемости L1-нормы $\\|\\mathbf{w}\\|_1$, и решается численно (обычно — координатным спуском).\n",
    "\n",
    "**Основная цель L1-регуляризации** — **отбор признаков**: обнуление малозначимых весов, что упрощает интерпретацию модели.  \n",
    "Особенно полезна, когда:\n",
    "- Число признаков велико ($p \\gg n$),\n",
    "- Многие признаки слабо влияют на целевую переменную.\n",
    "\n",
    "Однако Lasso склонен:\n",
    "- Обнулять один признак из группы коррелирующих,\n",
    "- Зависеть от масштаба признаков (требует стандартизации).\n",
    "\n",
    "#### Методы решения задачи Lasso (L2-ошибка + L1-регуляризация)\n",
    "\n",
    "| Метод | Основная идея | Скорость и устойчивость | Подходит для больших данных | Поддержка разреженности | Библиотеки и реализация | Особенности | Когда использовать | Ключевое различие с альтернативой |\n",
    "|-------|---------------|--------------------------|-------------------------------|--------------------------|----------------------------|-------------|----------------------|-------------------------------|\n",
    "| Coordinate Descent | Поочерёдная оптимизация одного веса при фиксированных остальных с учётом L1-штрафа | Высокая скорость, хорошая устойчивость | Да (при умеренном числе признаков) | Отлично | `scikit-learn`: `Lasso`, `LassoCV` | Эффективен и стабилен; используется по умолчанию | Для задач с отбором признаков, когда требуется разреженная модель | В отличие от ElasticNet, даёт **максимально разреженное решение**; не учитывает корреляцию между признаками |\n",
    "| ISTA / Proximal Gradient | Градиентный шаг + soft thresholding для L1-нормы | Средняя скорость, хорошая устойчивость | Да | Да | `scikit-learn`: нет прямого класса; альтернативы: `pylops`, `sporco`, ручная реализация | Базовый проксимальный метод; лежит в основе многих решателей | В исследованиях и кастомных моделях с L1-регуляризацией | Проксимальный оператор проще, чем в ElasticNet: **не включает L2-градиент** |\n",
    "| FISTA | Ускоренная версия ISTA (аналог метода Нестерова) | Высокая скорость, хорошая устойчивость | Да | Да | `scikit-learn`: нет встроенного класса; альтернативы: `pylops.FISTA`, `sporco`, `autograd`, `JAX` | Ускоренная сходимость; популярен в итерационной оптимизации | При разработке эффективных решателей, особенно в онлайн-режиме | Не включает L2-коррекцию в шаге, что делает его **менее устойчивым при мультиколлинеарности**, чем в ElasticNet |\n",
    "| LARS (Least Angle Regression) | Геометрический метод построения траектории коэффициентов с учётом корреляций | Средняя скорость, отличная устойчивость | Нет | Умеренная | `scikit-learn`: `LassoLars`, `LassoLarsCV`, `lars_path()` | Позволяет построить полный путь коэффициентов при росте \\( \\alpha \\) | Для анализа отбора признаков и визуализации \"Lasso path\" | **Уникален для Lasso**: в ElasticNet нет аналога, так как L1+L2 нарушает линейность пути |\n",
    "| ADMM (Alternating Direction Method of Multipliers) | Разделение задачи на подзадачи с двойственными переменными | Средняя скорость, отличная устойчивость | Да (в распределённых системах) | Да | `scikit-learn`: нет; альтернативы: `scikit-learn-extra`, `pyADMM`, `OSQP`, `CVXPY` | Масштабируемый; подходит для сложных ограничений | В промышленных системах и распределённых вычислениях | В Lasso — проще: **один проксимальный оператор (L1)**; в ElasticNet — сложнее из-за смешанного штрафа |\n",
    "| Subgradient Method | Использование субградиента из-за негладкости L1-нормы | Низкая скорость, удовлетворительная устойчивость | Да (но медленно) | Да | `scikit-learn`: нет реализации; альтернативы: ручная реализация на `NumPy`, `PyTorch`, `TensorFlow` | Прост в реализации, но неэффективен | Только для учебных целей или базового понимания | В Lasso — субградиент только от L1; в ElasticNet — сумма субградиента L1 и градиента L2 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Смешанная регуляризация (ElasticNet)\n",
    "\n",
    "Смешанная L1+L2-регуляризация объединяет преимущества Ridge и Lasso:\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\left( \\rho \\|\\mathbf{w}\\|_1 + (1 - \\rho) \\|\\mathbf{w}\\|_2^2 \\right)\n",
    "$\n",
    "\n",
    "где:\n",
    "- $\\alpha$ — общий коэффициент регуляризации,\n",
    "- $\\rho \\in [0, 1]$ — доля L1-регуляризации.\n",
    "\n",
    "Эта модель называется **ElasticNet**.\n",
    "\n",
    "**Преимущества перед Lasso**:\n",
    "- При наличии **групп коррелирующих признаков**, ElasticNet склонен сохранять их все (вместо выбора одного, как Lasso),\n",
    "- Более устойчива к шуму и переобучению,\n",
    "- Гибко балансирует между разреженностью (L1) и устойчивостью (L2).\n",
    "\n",
    "**Недостатки**:\n",
    "- Два гиперпараметра ($\\alpha$, $\\rho$), что усложняет подбор,\n",
    "- Требует масштабирования признаков,\n",
    "- Решается только итерационно.\n",
    "\n",
    "####  Методы решения задачи ElasticNet (L2-ошибка + L1 + L2-регуляризация)\n",
    "\n",
    "| Метод | Основная идея | Скорость и устойчивость | Подходит для больших данных | Поддержка разреженности | Библиотеки и реализация | Особенности | Когда использовать | Ключевое различие с альтернативой |\n",
    "|-------|---------------|--------------------------|-------------------------------|--------------------------|----------------------------|-------------|----------------------|-------------------------------|\n",
    "| Coordinate Descent | Поочерёдная оптимизация с учётом комбинированного штрафа \\( \\rho\\|w\\|_1 + (1-\\rho)\\|w\\|_2^2 \\) | Высокая скорость, хорошая устойчивость | Да (при умеренном числе признаков) | Хорошая (менее разреженная, чем у Lasso) | `scikit-learn`: `ElasticNet`, `ElasticNetCV` | Учитывает баланс между L1 и L2 через параметр \\( \\rho \\); устойчив при групповой корреляции | Когда признаки коррелируют группами, и нужен компромисс между отбором и устойчивостью | В отличие от Lasso, **стабилен при мультиколлинеарности**: L2-штраф предотвращает \"случайный\" отбор одного из группы коррелирующих признаков |\n",
    "| Proximal Gradient (ISTA) | Градиентный шаг + soft thresholding с поправкой на L2-компоненту | Средняя скорость, хорошая устойчивость | Да | Да | `scikit-learn`: нет прямого класса; альтернативы: `pylops`, `sporco`, ручная реализация | Требует модификации prox-оператора для смешанного штрафа | В исследованиях с составными регуляризаторами | Проксимальный оператор включает **оба штрафа**: soft thresholding для L1 и градиентное смещение от L2 |\n",
    "| FISTA | Ускоренная версия proximal gradient для смешанной регуляризации | Высокая скорость, хорошая устойчивость | Да | Да | `scikit-learn`: нет встроенного класса; альтернативы: `pylops.FISTA`, `sporco`, `autograd`, `JAX` | Ускоренная сходимость; эффективен при большом числе итераций | В кастомных моделях с быстрой сходимостью | Благодаря L2-штрафу — **более стабильная сходимость** по сравнению с Lasso при коррелирующих признаках |\n",
    "| ADMM (Alternating Direction Method of Multipliers) | Разделение задачи с учётом двух компонент регуляризации | Средняя скорость, отличная устойчивость | Да (в распределённых системах) | Да | `scikit-learn`: нет; альтернативы: `scikit-learn-extra`, `pyADMM`, `OSQP`, `CVXPY` | Гибкий; позволяет разделять L1 и L2-обновления | В промышленных масштабируемых системах | Позволяет **раздельно обрабатывать L1 и L2** компоненты, что невозможно в чистом Lasso |\n",
    "| Subgradient Method | Использование субградиента для L1-части при наличии L2-штрафа | Низкая скорость, удовлетворительная устойчивость | Да (но медленно) | Да | `scikit-learn`: нет реализации; альтернативы: ручная реализация на `NumPy`, `PyTorch`, `TensorFlow` | Прост, но неэффективен из-за медленной сходимости | Только для учебных целей | Субградиент — от L1, но **полный градиент включает L2**, что делает метод сложнее, чем в Lasso |\n",
    "\n",
    "### Итак\n",
    "\n",
    "Выбор метода регуляризации и алгоритма решения зависит от размера данных, структуры признаков и целей модели. Ridge подходит для устойчивости при мультиколлинеарности и имеет аналитические решатели, Lasso обеспечивает отбор признаков, но чувствителен к корреляциям, а ElasticNet объединяет преимущества обоих, балансируя между разреженностью и устойчивостью. Для малых данных предпочтительны аналитические методы (SVD, QR, Холецкого), а для больших — итерационные (SAG, SAGA, Coordinate Descent, SGD), требующие масштабирования признаков, но обеспечивающие масштабируемость и гибкость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 3 (LinearRegression):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные: один признак\n",
    "X_single = df_single[['x']] \n",
    "y_single = df_single['y']\n",
    "\n",
    "# Модель\n",
    "model = LinearRegression() # аналитический метод, матричное разложение SVD под капотом по умолчанию\n",
    "model.fit(X_single, y_single)\n",
    "\n",
    "# Результат\n",
    "m_sk = model.coef_[0]    # наклон\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМетод 3 (LinearRegression):\")\n",
    "print(f\"y = {m_sk:.3f}x + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков через матричную формулу:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Датасет (два признака):\n",
      "   x1  x2     y\n",
      "0   1   2   4.0\n",
      "1   2   3   7.0\n",
      "2   3   3   9.5\n",
      "3   4   4  12.5\n",
      "4   5   4  15.0\n",
      "5   6   5  18.0\n",
      "6   7   5  21.0\n",
      "7   8   5  24.0\n",
      "8   9   5  26.5\n",
      "9  10   5  29.0\n"
     ]
    }
   ],
   "source": [
    "data_multi = {\n",
    "    'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'x2': [2, 3, 3, 4, 4, 5, 5, 5, 5, 5],\n",
    "    'y':  [4.0, 7.0, 9.5, 12.5, 15.0, 18.0, 21.0, 24.0, 26.5, 29.0]\n",
    "}\n",
    "\n",
    "df_multi = pd.DataFrame(data_multi)\n",
    "print(\"\\nДатасет (два признака):\")\n",
    "print(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (матричная формула):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "n = len(df_multi)\n",
    "\n",
    "# Матрица X: [1, x1, x2]\n",
    "X_multi = np.column_stack([np.ones(n), df_multi['x1'], df_multi['x2']])\n",
    "y_multi = df_multi['y'].values\n",
    "\n",
    "# Решение: w = (X^T X)^{-1} X^T y\n",
    "w_multi = np.linalg.inv(X_multi.T @ X_multi) @ X_multi.T @ y_multi\n",
    "\n",
    "b_multi, w1, w2 = w_multi[0], w_multi[1], w_multi[2]\n",
    "\n",
    "print(f\"\\nМножественная регрессия (матричная формула):\")\n",
    "print(f\"y = {w1:.3f}·x1 + {w2:.3f}·x2 + {b_multi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков из \"коробки\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (LinearRegression):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные\n",
    "X_multi = df_multi[['x1', 'x2']]  # признаки\n",
    "y_multi = df_multi['y']           # целевая переменная\n",
    "\n",
    "# Модель \"из коробки\"\n",
    "model = LinearRegression()\n",
    "model.fit(X_multi, y_multi)\n",
    "\n",
    "# Получаем коэффициенты\n",
    "w1_sk = model.coef_[0]   # коэффициент при x1\n",
    "w2_sk = model.coef_[1]   # коэффициент при x2\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМножественная регрессия (LinearRegression):\")\n",
    "print(f\"y = {w1_sk:.3f}·x1 + {w2_sk:.3f}·x2 + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В `LinearRegression` из `scikit-learn` нельзя напрямую выбрать метод решения (например, SVD или QR).  \n",
    "\n",
    "Однако по умолчанию используется SVD-подход через LAPACK-решатель `gelsd`, который:\n",
    "\n",
    "- Устойчив к вырожденным и плохо обусловленным матрицам,\n",
    "- Корректно работает при $ n < p $ (больше признаков, чем объектов),\n",
    "- Обеспечивает высокую численную стабильность.\n",
    "\n",
    "Под капотом: вызывается `scipy.linalg.lstsq` с `lapack_driver='gelsd'` — метод на основе сингулярного разложения с divide-and-conquer.\n",
    "\n",
    "\n",
    "### LAPACK-решатели, используемые для МНК\n",
    "\n",
    "Вот три основных драйвера LAPACK, доступных в `scipy.linalg.lstsq`, которые могут использоваться для решения задачи $\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2$:\n",
    "\n",
    "| Метод   | Основа         | Устойчивость       | Скорость     | Когда используется |\n",
    "|--------|----------------|--------------------|--------------|----------------------|\n",
    "| `gelsd` | SVD + divide-and-conquer | Очень высокая | Средняя / высокая | По умолчанию в `scikit-learn` и `scipy` |\n",
    "| `gelss` | Классическое SVD | Высокая         | Медленная  | Устаревшие системы, ограниченная память |\n",
    "| `gelsy` | QR с поворотом по столбцам | Средняя (требует полного ранга) | Высокая | Если матрица $\\mathbf{X}$ имеет полный ранг и важна скорость |\n",
    "\n",
    "\n",
    "`LinearRegression` всегда использует `gelsd`, если не указано иное — это делает его наиболее универсальным и надёжным выбором для реальных данных.\n",
    "\n",
    "\n",
    "### Почему нельзя выбрать метод вручную?\n",
    "\n",
    "В отличие от `Ridge` или `LogisticRegression`, у `LinearRegression` нет параметра `solver`.  \n",
    "Это сделано по следующим причинам:\n",
    "\n",
    "1. **Универсальность**  \n",
    "   SVD (`gelsd`) работает во всех случаях — даже если матрица $\\mathbf{X}$ вырождена, имеет мультиколлинеарность или число признаков превышает число объектов.\n",
    "\n",
    "2. **Надёжность \"из коробки\"**  \n",
    "   `scikit-learn` стремится к простоте и предсказуемости: пользователь должен получать устойчивый результат без необходимости настраивать низкоуровневые параметры.\n",
    "\n",
    "3. **Минимизация ошибок пользователя**  \n",
    "   Если бы был выбор между SVD и QR, пользователь мог бы выбрать QR на плохих данных — получить нестабильные веса или не понять, почему модель ведёт себя странно.\n",
    "\n",
    "4. **Производительность vs стабильность**  \n",
    "   QR быстрее на хорошо обусловленных данных.  \n",
    "   SVD (`gelsd`) — оптимальный компромисс между скоростью и устойчивостью для большинства реальных задач.\n",
    "\n",
    "\n",
    "### QR-разложение напрямую можно реализовать через `scipy`:\n",
    "\n",
    "```python\n",
    "from scipy.linalg import qr, solve_triangular\n",
    "\n",
    "Q, R = qr(X, mode='economic')\n",
    "w = solve_triangular(R, Q.T @ y)  # w = R^{-1} Q^T y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
