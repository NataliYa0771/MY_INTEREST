{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аналитические и численные методы линейной регрессии\n",
    "\n",
    "В этом ноутбуке рассматриваются различные подходы к решению задачи линейной регрессии с помощью **аналитических (непараметрических) методов** и параметрического метода LinearRegression из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет (один признак):\n",
      "    x     y\n",
      "0   1   3.5\n",
      "1   2   6.8\n",
      "2   3   9.2\n",
      "3   4  12.0\n",
      "4   5  15.1\n",
      "5   6  18.3\n",
      "6   7  21.0\n",
      "7   8  24.2\n",
      "8   9  27.1\n",
      "9  10  30.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Датасет: 10 наблюдений, один признак\n",
    "data_single = {\n",
    "    'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'y': [3.5, 6.8, 9.2, 12.0, 15.1, 18.3, 21.0, 24.2, 27.1, 30.5]\n",
    "}\n",
    "\n",
    "df_single = pd.DataFrame(data_single)\n",
    "print(\"Датасет (один признак):\")\n",
    "print(df_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Аналитическое решение парной линейной регрессии\n",
    "\n",
    "Для случая **ОДНОГО** признака $ x $ и целевой переменной $ y $ можно найти оптимальные параметры модели $ y = mx + b $ **аналитически**, без итераций.\n",
    "\n",
    "###  Цель\n",
    "Найти такие $ m $ (наклон) и $ b $ (свободный член), которые минимизируют сумму квадратов ошибок (MSE).\n",
    "\n",
    "---\n",
    "\n",
    "###  Формулы МНК (метод наименьших квадратов)\n",
    "\n",
    "Для $ n $ наблюдений:\n",
    "\n",
    "- **Наклон $ m $:**\n",
    "  $$\n",
    "  m = \\frac{n \\sum (x_i y_i) - \\sum x_i \\sum y_i}{n \\sum (x_i^2) - (\\sum x_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Свободный член $ b $:**\n",
    "  $$\n",
    "  b = \\bar{y} - m \\bar{x}\n",
    "  $$\n",
    "  где $ \\bar{x} $ и $ \\bar{y} $ — средние значения.\n",
    "\n",
    "Эта формула — частный случай метода наименьших квадратов (МНК), выведенный только для случая одного признака x .\n",
    "Она не работает при двух и более признаках, потому что: \n",
    "\n",
    "- теряется возможность учитывать взаимное влияние признаков,\n",
    "- нельзя выразить несколько коэффициентов в одной скалярной формуле.\n",
    "\n",
    "\n",
    "###  Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 1 (ручная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "points = list(df_single.itertuples(index=False))\n",
    "\n",
    "n = len(points)\n",
    "\n",
    "# Вычисляем m (наклон)\n",
    "m = (n * sum(p.x * p.y for p in points) - sum(p.x for p in points) * sum(p.y for p in points)) / \\\n",
    "    (n * sum(p.x**2 for p in points) - (sum(p.x for p in points))**2)\n",
    "\n",
    "# Вычисляем b (свободный член)\n",
    "b = (sum(p.y for p in points) / n) - m * (sum(p.x for p in points) / n)\n",
    "\n",
    "print(f\"\\nМетод 1 (ручная формула):\")\n",
    "print(f\"y = {m:.3f}x + {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Аналитическое (матричное) решение парной и множественной линейной регрессии (МНК)\n",
    "\n",
    "Этот метод позволяет найти оптимальные параметры модели \\( y = mx + b \\) аналитически, используя линейную алгебру. Он работает не только для одного признака, но и для любого числа признаков — это универсальный подход, основанный на методе наименьших квадратов (МНК).\n",
    "\n",
    "### Математическая формула\n",
    "\n",
    "Оптимальные параметры $ \\mathbf{w} $ находятся по формуле метода наименьших квадратов:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $ X $ — матрица признаков с добавленным столбцом единиц (для свободного члена $ b $)\n",
    "- $ \\mathbf{y} $ — вектор целевых значений\n",
    "- $ \\mathbf{w} = [b, m] $ — вектор параметров модели\n",
    "\n",
    "\n",
    "Хотя в теории решение выражается формулой, на практике в библиотеках, включая `scikit-learn`, она **не используется напрямую** из-за численной неустойчивости и может приводить к значительным ошибкам в присутствии мультиколлинеарности, шума или при большом количестве признаков.\n",
    "\n",
    "### Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 2 (матричная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(n), df_single['x']])  # [1, x]\n",
    "y = df_single['y'].values\n",
    "\n",
    "# Аналитическое решение: w = (X^T X)^{-1} X^T y\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "b_mnk, m_mnk = w[0], w[1]\n",
    "\n",
    "print(f\"\\nМетод 2 (матричная формула):\")\n",
    "print(f\"y = {m_mnk:.3f}x + {b_mnk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Метод  LinearRegression из  scikit-learn\n",
    "\n",
    "Внутри вызывается функция `numpy.linalg.lstsq` или аналог из `scipy`, которая решает задачу наименьших квадратов по умолчанию с помощью SVD.\n",
    "\n",
    "Матрица $ X $ представляется в виде:\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "где:\n",
    "- $ U $ и $ V $ — ортогональные матрицы,\n",
    "- $ \\Sigma $ — диагональная матрица сингулярных значений.\n",
    "\n",
    "Решение для вектора весов $ \\mathbf{w} $ находится как:\n",
    "$$\n",
    "\\mathbf{w} = V \\Sigma^{-1} U^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "### Почему не используется формула с обратной матрицей?\n",
    "\n",
    "Несмотря на математическую корректность формулы $ (X^T X)^{-1} X^T y $, её применение в реальных условиях затруднено по следующим причинам:\n",
    "\n",
    "1. **Численная неустойчивость**  \n",
    "   При близкой к линейной зависимости признаков (мультиколлинеарность) матрица $ X^T X $ становится плохо обусловленной, и её обращение приводит к большим ошибкам.\n",
    "\n",
    "2. **Потеря точности при умножении**  \n",
    "   Вычисление $ X^T X $ удваивает ошибку округления, что особенно критично при большом масштабе данных.\n",
    "\n",
    "3. **Ограничения по размерности**  \n",
    "   Если число признаков превышает число наблюдений, матрица $ X^T X $ вырождена и необратима.\n",
    "\n",
    "4. **Риск переполнения**  \n",
    "   При больших значениях элементов $ X $ произведение $ X^T X $ может привести к переполнению.\n",
    "\n",
    "\n",
    "### Преимущества использования SVD\n",
    "\n",
    "- Высокая численная устойчивость.\n",
    "- Возможность работы с вырожденными и плохо обусловленными матрицами.\n",
    "- Поддержка случаев, когда число признаков больше числа объектов.\n",
    "- Возможность контроля устойчивости через параметр `rcond` (порог усечения малых сингулярных значений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 3 (LinearRegression):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные: один признак\n",
    "X_single = df_single[['x']] \n",
    "y_single = df_single['y']\n",
    "\n",
    "# Модель\n",
    "model = LinearRegression()\n",
    "model.fit(X_single, y_single)\n",
    "\n",
    "# Результат\n",
    "m_sk = model.coef_[0]    # наклон\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМетод 3 (LinearRegression):\")\n",
    "print(f\"y = {m_sk:.3f}x + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков через матричную формулу:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Датасет (два признака):\n",
      "   x1  x2     y\n",
      "0   1   2   4.0\n",
      "1   2   3   7.0\n",
      "2   3   3   9.5\n",
      "3   4   4  12.5\n",
      "4   5   4  15.0\n",
      "5   6   5  18.0\n",
      "6   7   5  21.0\n",
      "7   8   5  24.0\n",
      "8   9   5  26.5\n",
      "9  10   5  29.0\n"
     ]
    }
   ],
   "source": [
    "data_multi = {\n",
    "    'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'x2': [2, 3, 3, 4, 4, 5, 5, 5, 5, 5],\n",
    "    'y':  [4.0, 7.0, 9.5, 12.5, 15.0, 18.0, 21.0, 24.0, 26.5, 29.0]\n",
    "}\n",
    "\n",
    "df_multi = pd.DataFrame(data_multi)\n",
    "print(\"\\nДатасет (два признака):\")\n",
    "print(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (матричная формула):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "n = len(df_multi)\n",
    "\n",
    "# Матрица X: [1, x1, x2]\n",
    "X_multi = np.column_stack([np.ones(n), df_multi['x1'], df_multi['x2']])\n",
    "y_multi = df_multi['y'].values\n",
    "\n",
    "# Решение: w = (X^T X)^{-1} X^T y\n",
    "w_multi = np.linalg.inv(X_multi.T @ X_multi) @ X_multi.T @ y_multi\n",
    "\n",
    "b_multi, w1, w2 = w_multi[0], w_multi[1], w_multi[2]\n",
    "\n",
    "print(f\"\\nМножественная регрессия (матричная формула):\")\n",
    "print(f\"y = {w1:.3f}·x1 + {w2:.3f}·x2 + {b_multi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков из \"коробки\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (LinearRegression):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные\n",
    "X_multi = df_multi[['x1', 'x2']]  # признаки\n",
    "y_multi = df_multi['y']           # целевая переменная\n",
    "\n",
    "# Модель \"из коробки\"\n",
    "model = LinearRegression()\n",
    "model.fit(X_multi, y_multi)\n",
    "\n",
    "# Получаем коэффициенты\n",
    "w1_sk = model.coef_[0]   # коэффициент при x1\n",
    "w2_sk = model.coef_[1]   # коэффициент при x2\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМножественная регрессия (LinearRegression):\")\n",
    "print(f\"y = {w1_sk:.3f}·x1 + {w2_sk:.3f}·x2 + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
