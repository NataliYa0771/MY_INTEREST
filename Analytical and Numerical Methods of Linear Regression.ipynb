{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аналитические и численные методы линейной регрессии\n",
    "\n",
    "### Постановка задачи и выбор функции потерь\n",
    "\n",
    "В этом ноутбуке рассматриваются различные подходы к решению задачи **линейной регрессии** с использованием **L2-потери** (квадратичной ошибки):\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2,\n",
    "$$\n",
    "где:\n",
    "- $\\mathbf{X}$ — матрица признаков,\n",
    "- $\\mathbf{w}$ — вектор весов (параметров модели),\n",
    "- $\\mathbf{y}$ — вектор целевой переменной.\n",
    "\n",
    "Эта постановка известна как **метод наименьших квадратов (OLS)**. Одно из её ключевых преимуществ — существование **аналитического решения в замкнутой форме**:\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "при условии, что матрица $\\mathbf{X}^T\\mathbf{X}$ обратима. Это позволяет находить оптимальные веса без итераций — как в случае одного признака, так и нескольких.\n",
    "\n",
    "Мы сравниваем:\n",
    "- Аналитические (непараметрические) решения вручную,\n",
    "- Реализацию `LinearRegression` из `scikit-learn`, которая использует SVD-разложение для численно устойчивого решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему мы не рассматриваем минимизацию $ \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1 $?\n",
    "\n",
    "В этом ноутбуке мы фокусируемся на задаче линейной регрессии с **L2-потерей** (метод наименьших квадратов),\n",
    "для которой существуют **аналитические решения** — как в случае одного признака, так и в многомерном случае. \n",
    "Однако, если вместо L2 использовать **L1-норму**:\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1,\n",
    "$$\n",
    "— **аналитического решения в замкнутой форме не существует**. Причина в том, что L1-норма не является гладкой функцией (её производная не определена при нулевых остатках), и стандартные методы математического анализа для поиска минимума через приравнивание градиента к нулю неприменимы.\n",
    "\n",
    "- **Задача минимизации L1-потери** (т.е. $\\min \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1$) **не имеет аналитического решения**, и на практике решается следующими способами:\n",
    "\n",
    "  1. **\"Из коробки\"**:\n",
    "     - `QuantileRegressor(quantile=0.5)` — в `scikit-learn`, эквивалентна L1-регрессии.\n",
    "     - `LADRegressor` — начиная с `scikit-learn >= 1.3`, прямая реализация.\n",
    "     > Оба метода используют **линейное программирование** под капотом (например, солвер `highs`).\n",
    "\n",
    "  2. **Через `scipy.optimize.linprog`**:\n",
    "     - Можно сформулировать задачу как LP вручную.\n",
    "     - Точно, но требует ручного построения матриц ограничений.\n",
    "     - Подходит для малых задач.\n",
    "\n",
    "  3. **Субградиентный спуск вручную**:\n",
    "     - Реализуется с использованием `np.sign(остатки)`.\n",
    "     - Просто, но медленно и требует подбора шага.\n",
    "     - Нет в `SGDRegressor` — нужно писать самому.\n",
    "\n",
    "- **Добавление L1 или L2-регуляризации к L1-потере**:\n",
    "  - Теоретически возможно:  \n",
    "    $\n",
    "    \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1 + \\alpha_1 \\|\\mathbf{w}\\|_1 + \\alpha_2 \\|\\mathbf{w}\\|_2^2\n",
    "    $\n",
    "  - **Но на практике**:\n",
    "    - Не реализовано в `scikit-learn`,\n",
    "    - Не используется массово,\n",
    "    - Реализуется **вручную** через библиотеки вроде `cvxpy`.\n",
    " \n",
    "> Для L1-потери **лучше использовать `QuantileRegressor` или `LADRegressor` из коробки**.  \n",
    "> Всё остальное — либо ручная реализация, либо специализированные инструменты вроде `cvxpy`, которые нужны только в особых случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет (один признак):\n",
      "    x     y\n",
      "0   1   3.5\n",
      "1   2   6.8\n",
      "2   3   9.2\n",
      "3   4  12.0\n",
      "4   5  15.1\n",
      "5   6  18.3\n",
      "6   7  21.0\n",
      "7   8  24.2\n",
      "8   9  27.1\n",
      "9  10  30.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Датасет: 10 наблюдений, один признак\n",
    "data_single = {\n",
    "    'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'y': [3.5, 6.8, 9.2, 12.0, 15.1, 18.3, 21.0, 24.2, 27.1, 30.5]\n",
    "}\n",
    "\n",
    "df_single = pd.DataFrame(data_single)\n",
    "print(\"Датасет (один признак):\")\n",
    "print(df_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Аналитическое решение парной линейной регрессии\n",
    "\n",
    "Для случая **ОДНОГО** признака $ x $ и целевой переменной $ y $ можно найти оптимальные параметры модели $ y = mx + b $ **аналитически**, без итераций.\n",
    "\n",
    "###  Цель\n",
    "Найти такие $ m $ (наклон) и $ b $ (свободный член), которые минимизируют сумму квадратов ошибок (MSE).\n",
    "\n",
    "---\n",
    "\n",
    "###  Формулы МНК (метод наименьших квадратов)\n",
    "\n",
    "Для $ n $ наблюдений:\n",
    "\n",
    "- **Наклон $ m $:**\n",
    "  $$\n",
    "  m = \\frac{n \\sum (x_i y_i) - \\sum x_i \\sum y_i}{n \\sum (x_i^2) - (\\sum x_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Свободный член $ b $:**\n",
    "  $$\n",
    "  b = \\bar{y} - m \\bar{x}\n",
    "  $$\n",
    "  где $ \\bar{x} $ и $ \\bar{y} $ — средние значения.\n",
    "\n",
    "Эта формула — частный случай метода наименьших квадратов (МНК), выведенный только для случая одного признака x .\n",
    "Она не работает при двух и более признаках, потому что: \n",
    "\n",
    "- теряется возможность учитывать взаимное влияние признаков,\n",
    "- нельзя выразить несколько коэффициентов в одной скалярной формуле.\n",
    "\n",
    "\n",
    "###  Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 1 (ручная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "points = list(df_single.itertuples(index=False))\n",
    "\n",
    "n = len(points)\n",
    "\n",
    "# Вычисляем m (наклон)\n",
    "m = (n * sum(p.x * p.y for p in points) - sum(p.x for p in points) * sum(p.y for p in points)) / \\\n",
    "    (n * sum(p.x**2 for p in points) - (sum(p.x for p in points))**2)\n",
    "\n",
    "# Вычисляем b (свободный член)\n",
    "b = (sum(p.y for p in points) / n) - m * (sum(p.x for p in points) / n)\n",
    "\n",
    "print(f\"\\nМетод 1 (ручная формула):\")\n",
    "print(f\"y = {m:.3f}x + {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Аналитическое (матричное) решение парной и множественной линейной регрессии (МНК)\n",
    "\n",
    "Этот метод позволяет найти оптимальные параметры модели \\( y = mx + b \\) аналитически, используя линейную алгебру. Он работает не только для одного признака, но и для любого числа признаков — это универсальный подход, основанный на методе наименьших квадратов (МНК).\n",
    "\n",
    "### Математическая формула\n",
    "\n",
    "Оптимальные параметры $ \\mathbf{w} $ находятся по формуле метода наименьших квадратов:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $ X $ — матрица признаков с добавленным столбцом единиц (для свободного члена $ b $)\n",
    "- $ \\mathbf{y} $ — вектор целевых значений\n",
    "- $ \\mathbf{w} = [b, m] $ — вектор параметров модели\n",
    "\n",
    "\n",
    "Хотя в теории решение выражается формулой, на практике в библиотеках, включая `scikit-learn`, она **не используется напрямую** из-за численной неустойчивости и может приводить к значительным ошибкам в присутствии мультиколлинеарности, шума или при большом количестве признаков.\n",
    "\n",
    "### Реализация формулы в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 2 (матричная формула):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(n), df_single['x']])  # [1, x]\n",
    "y = df_single['y'].values\n",
    "\n",
    "# Аналитическое решение: w = (X^T X)^{-1} X^T y\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "b_mnk, m_mnk = w[0], w[1]\n",
    "\n",
    "print(f\"\\nМетод 2 (матричная формула):\")\n",
    "print(f\"y = {m_mnk:.3f}x + {b_mnk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Метод  LinearRegression из  scikit-learn\n",
    "\n",
    "Внутри вызывается функция `numpy.linalg.lstsq` или аналог из `scipy`, которая решает задачу наименьших квадратов по умолчанию с помощью SVD.\n",
    "\n",
    "Матрица $ X $ представляется в виде:\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "где:\n",
    "- $ U $ и $ V $ — ортогональные матрицы,\n",
    "- $ \\Sigma $ — диагональная матрица сингулярных значений.\n",
    "\n",
    "Решение для вектора весов $ \\mathbf{w} $ находится как:\n",
    "$$\n",
    "\\mathbf{w} = V \\Sigma^{-1} U^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "### Почему не используется формула с обратной матрицей?\n",
    "\n",
    "Несмотря на математическую корректность формулы $ (X^T X)^{-1} X^T y $, её применение в реальных условиях затруднено по следующим причинам:\n",
    "\n",
    "1. **Численная неустойчивость**  \n",
    "   При близкой к линейной зависимости признаков (мультиколлинеарность) матрица $ X^T X $ становится плохо обусловленной, и её обращение приводит к большим ошибкам.\n",
    "\n",
    "2. **Потеря точности при умножении**  \n",
    "   Вычисление $ X^T X $ удваивает ошибку округления, что особенно критично при большом масштабе данных.\n",
    "\n",
    "3. **Ограничения по размерности**  \n",
    "   Если число признаков превышает число наблюдений, матрица $ X^T X $ вырождена и необратима.\n",
    "\n",
    "4. **Риск переполнения**  \n",
    "   При больших значениях элементов $ X $ произведение $ X^T X $ может привести к переполнению.\n",
    "\n",
    "\n",
    "### Преимущества использования SVD\n",
    "\n",
    "- Высокая численная устойчивость.\n",
    "- Возможность работы с вырожденными и плохо обусловленными матрицами.\n",
    "- Поддержка случаев, когда число признаков больше числа объектов.\n",
    "- Возможность контроля устойчивости через параметр `rcond` (порог усечения малых сингулярных значений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы решения задачи линейной регрессии в `scikit-learn`\n",
    "\n",
    "В данном исследовании мы используем **SVD-решение** (`LinearRegression`), но в `scikit-learn` также доступны другие аналитические методы для решения задачи:\n",
    "\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_2^2\n",
    "$\n",
    "\n",
    "#### 1. Основные подходы\n",
    "\n",
    "- **`LinearRegression`**  \n",
    "  - **Алгоритм**: SVD или QR-разложение  \n",
    "  - **Формула решения**:  \n",
    "   $ \n",
    "    \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "    \\$ \n",
    "    \n",
    "  - **Особенности**:  \n",
    "    - Нет регуляризации ($\\alpha = 0$)  \n",
    "    - Устойчив к вырожденным или плохо обусловленным матрицам за счёт SVD  \n",
    "    - Работает даже при линейной зависимости признаков  \n",
    "\n",
    "- **`Ridge` с `solver='svd'`**  \n",
    "  - **Формула**:  \n",
    "    $\n",
    "    \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "    $ \n",
    "  - **Отличие**: добавлена **L2-регуляризация** (параметр `alpha > 0`)  \n",
    "  - Улучшает устойчивость при мультиколлинеарности  \n",
    "\n",
    "#### 2. Альтернативные аналитические методы\n",
    "\n",
    "- **`Ridge` с `solver='cholesky'`**  \n",
    "  - Использует разложение Холецкого для $\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I}$  \n",
    "  - **Плюсы**: высокая скорость при малом числе признаков  \n",
    "  - **Минусы**: требует положительной определённости матрицы (регуляризация обязательна)  \n",
    "\n",
    "- **`Ridge` с `solver='lsqr'`**  \n",
    "  - Использует итерационный метод на основе QR-разложения  \n",
    "  - Поддерживает разреженные матрицы  \n",
    "  - Эффективен при большом числе признаков или наблюдений  \n",
    "\n",
    "#### 3. Сравнительная таблица методов\n",
    "\n",
    "| Метод                     | Алгоритм          | Регуляризация | Устойчивость       | Скорость     |\n",
    "|---------------------------|-------------------|---------------|--------------------|--------------|\n",
    "| `LinearRegression`        | SVD / QR          | Нет           | Высокая            | Средняя      |\n",
    "| `Ridge(solver='svd')`     | SVD               | L2            | Очень высокая      | Медленная    |\n",
    "| `Ridge(solver='cholesky')`| Холецкого         | L2            | Средняя (с $\\alpha$)| Быстрая      |\n",
    "| `Ridge(solver='lsqr')`    | Итеративный QR    | L2            | Высокая            | Очень быстрая|\n",
    "\n",
    "> Все перечисленные методы:\n",
    "> - Являются **аналитическими** (или основаны на прямых матричных разложениях)  \n",
    "> - Решают обобщённую задачу минимизации:  \n",
    ">   $\n",
    "   \\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_2^2\n",
    "   $\n",
    "> - **Не требуют подбора шага обучения** или числа итераций  \n",
    "> - Отличаются по устойчивости, скорости и требованиям к данным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неаналитические методы для задачи L2-потери\n",
    "\n",
    "Хотя для задачи минимизации L2-потери  \n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2\n",
    "$\n",
    "существуют аналитические решения (например, через SVD), при больших объёмах данных они становятся вычислительно затратными.  \n",
    "В таких случаях используются **итерационные (неаналитические) методы**, не требующие обращения матриц.\n",
    "\n",
    "| Метод                     | Алгоритм                     | Регуляризация | Подходит для больших данных | Особенности |\n",
    "|--------------------------|-------------------------------|---------------|------------------------------|-------------|\n",
    "| `Ridge(solver='sag')`     | Стохастический усреднённый градиент (SAG) | L2       | Да (особенно при $n, p > 10^4$) | Быстрый сходимость, но требует масштабирования признаков |\n",
    "| `Ridge(solver='saga')`    | Улучшенный SAG (SAGA)         | L2            | Да                         | Поддерживает L1 и смешанную регуляризацию, более устойчив |\n",
    "| `SGDRegressor` с квадратичной ошибкой | Стохастический градиентный спуск (SGD) | L2 (опционально) | Да (очень большие выборки) | Высокая гибкость, требует подбора шага обучения |\n",
    "\n",
    "> Все перечисленные методы:\n",
    "> - Являются **итерационными**, а не аналитическими,\n",
    "> - Эффективны при большом числе объектов или признаков,\n",
    "> - Основаны на приближённом вычислении градиента по подвыборкам,\n",
    "> - Требуют **предварительного масштабирования признаков** для стабильной сходимости.\n",
    "\n",
    "---\n",
    "\n",
    "### L1-регуляризация (Lasso)\n",
    "\n",
    "L1-регуляризация добавляется к L2-потере для получения разреженных решений:\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\|\\mathbf{w}\\|_1\n",
    "$\n",
    "\n",
    "Эта задача известна как **Lasso-регрессия**.  \n",
    "Она не имеет аналитического решения из-за недифференцируемости L1-нормы $\\|\\mathbf{w}\\|_1$, и решается численно (обычно — координатным спуском).\n",
    "\n",
    "**Основная цель L1-регуляризации** — **отбор признаков**: обнуление малозначимых весов, что упрощает интерпретацию модели.  \n",
    "Особенно полезна, когда:\n",
    "- Число признаков велико ($p \\gg n$),\n",
    "- Многие признаки слабо влияют на целевую переменную.\n",
    "\n",
    "Однако Lasso склонен:\n",
    "- Обнулять один признак из группы коррелирующих,\n",
    "- Зависеть от масштаба признаков (требует стандартизации).\n",
    "\n",
    "---\n",
    "\n",
    "### Смешанная регуляризация (ElasticNet)\n",
    "\n",
    "Смешанная L1+L2-регуляризация объединяет преимущества Ridge и Lasso:\n",
    "$\n",
    "\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\alpha \\left( \\rho \\|\\mathbf{w}\\|_1 + (1 - \\rho) \\|\\mathbf{w}\\|_2^2 \\right)\n",
    "$\n",
    "\n",
    "где:\n",
    "- $\\alpha$ — общий коэффициент регуляризации,\n",
    "- $\\rho \\in [0, 1]$ — доля L1-регуляризации.\n",
    "\n",
    "Эта модель называется **ElasticNet**.\n",
    "\n",
    "**Преимущества перед Lasso**:\n",
    "- При наличии **групп коррелирующих признаков**, ElasticNet склонен сохранять их все (вместо выбора одного, как Lasso),\n",
    "- Более устойчива к шуму и переобучению,\n",
    "- Гибко балансирует между разреженностью (L1) и устойчивостью (L2).\n",
    "\n",
    "**Недостатки**:\n",
    "- Два гиперпараметра ($\\alpha$, $\\rho$), что усложняет подбор,\n",
    "- Требует масштабирования признаков,\n",
    "- Решается только итерационно.\n",
    "\n",
    "\n",
    "- **Неаналитические методы** (SAG, SAGA, SGD) — эффективны для **больших данных**, где аналитические решения неприменимы.\n",
    "- **L1-регуляризация** позволяет строить **разреженные и интерпретируемые модели**, но не имеет замкнутой формы.\n",
    "- **Смешанная регуляризация (ElasticNet)** — гибкий компромисс между Lasso и Ridge, особенно полезный при коррелирующих признаках.\n",
    "\n",
    "> Все эти подходы основаны на **численной оптимизации** и требуют **предварительной обработки данных**, но дают значительный выигрыш в гибкости и масштабируемости по сравнению с аналитическими методами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метод 3 (LinearRegression):\n",
      "y = 2.972x + 0.427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные: один признак\n",
    "X_single = df_single[['x']] \n",
    "y_single = df_single['y']\n",
    "\n",
    "# Модель\n",
    "model = LinearRegression() # аналитический метод, матричное разложение SVD под капотом по умолчанию\n",
    "model.fit(X_single, y_single)\n",
    "\n",
    "# Результат\n",
    "m_sk = model.coef_[0]    # наклон\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМетод 3 (LinearRegression):\")\n",
    "print(f\"y = {m_sk:.3f}x + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков через матричную формулу:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Датасет (два признака):\n",
      "   x1  x2     y\n",
      "0   1   2   4.0\n",
      "1   2   3   7.0\n",
      "2   3   3   9.5\n",
      "3   4   4  12.5\n",
      "4   5   4  15.0\n",
      "5   6   5  18.0\n",
      "6   7   5  21.0\n",
      "7   8   5  24.0\n",
      "8   9   5  26.5\n",
      "9  10   5  29.0\n"
     ]
    }
   ],
   "source": [
    "data_multi = {\n",
    "    'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'x2': [2, 3, 3, 4, 4, 5, 5, 5, 5, 5],\n",
    "    'y':  [4.0, 7.0, 9.5, 12.5, 15.0, 18.0, 21.0, 24.0, 26.5, 29.0]\n",
    "}\n",
    "\n",
    "df_multi = pd.DataFrame(data_multi)\n",
    "print(\"\\nДатасет (два признака):\")\n",
    "print(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (матричная формула):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "n = len(df_multi)\n",
    "\n",
    "# Матрица X: [1, x1, x2]\n",
    "X_multi = np.column_stack([np.ones(n), df_multi['x1'], df_multi['x2']])\n",
    "y_multi = df_multi['y'].values\n",
    "\n",
    "# Решение: w = (X^T X)^{-1} X^T y\n",
    "w_multi = np.linalg.inv(X_multi.T @ X_multi) @ X_multi.T @ y_multi\n",
    "\n",
    "b_multi, w1, w2 = w_multi[0], w_multi[1], w_multi[2]\n",
    "\n",
    "print(f\"\\nМножественная регрессия (матричная формула):\")\n",
    "print(f\"y = {w1:.3f}·x1 + {w2:.3f}·x2 + {b_multi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Пример для ДВУХ признаков из \"коробки\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Множественная регрессия (LinearRegression):\n",
      "y = 2.752·x1 + 0.154·x2 + 0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Данные\n",
    "X_multi = df_multi[['x1', 'x2']]  # признаки\n",
    "y_multi = df_multi['y']           # целевая переменная\n",
    "\n",
    "# Модель \"из коробки\"\n",
    "model = LinearRegression()\n",
    "model.fit(X_multi, y_multi)\n",
    "\n",
    "# Получаем коэффициенты\n",
    "w1_sk = model.coef_[0]   # коэффициент при x1\n",
    "w2_sk = model.coef_[1]   # коэффициент при x2\n",
    "b_sk = model.intercept_  # свободный член\n",
    "\n",
    "print(f\"\\nМножественная регрессия (LinearRegression):\")\n",
    "print(f\"y = {w1_sk:.3f}·x1 + {w2_sk:.3f}·x2 + {b_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В `LinearRegression` из `scikit-learn` нельзя напрямую выбрать метод решения (например, SVD или QR).  \n",
    "\n",
    "Однако по умолчанию используется SVD-подход через LAPACK-решатель `gelsd`, который:\n",
    "\n",
    "- Устойчив к вырожденным и плохо обусловленным матрицам,\n",
    "- Корректно работает при $ n < p $ (больше признаков, чем объектов),\n",
    "- Обеспечивает высокую численную стабильность.\n",
    "\n",
    "Под капотом: вызывается `scipy.linalg.lstsq` с `lapack_driver='gelsd'` — метод на основе сингулярного разложения с divide-and-conquer.\n",
    "\n",
    "\n",
    "### LAPACK-решатели, используемые для МНК\n",
    "\n",
    "Вот три основных драйвера LAPACK, доступных в `scipy.linalg.lstsq`, которые могут использоваться для решения задачи $\\min_{\\mathbf{w}} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2$:\n",
    "\n",
    "| Метод   | Основа         | Устойчивость       | Скорость     | Когда используется |\n",
    "|--------|----------------|--------------------|--------------|----------------------|\n",
    "| `gelsd` | SVD + divide-and-conquer | Очень высокая | Средняя / высокая | По умолчанию в `scikit-learn` и `scipy` |\n",
    "| `gelss` | Классическое SVD | Высокая         | Медленная  | Устаревшие системы, ограниченная память |\n",
    "| `gelsy` | QR с поворотом по столбцам | Средняя (требует полного ранга) | Высокая | Если матрица $\\mathbf{X}$ имеет полный ранг и важна скорость |\n",
    "\n",
    "\n",
    "`LinearRegression` всегда использует `gelsd`, если не указано иное — это делает его наиболее универсальным и надёжным выбором для реальных данных.\n",
    "\n",
    "\n",
    "### Почему нельзя выбрать метод вручную?\n",
    "\n",
    "В отличие от `Ridge` или `LogisticRegression`, у `LinearRegression` нет параметра `solver`.  \n",
    "Это сделано по следующим причинам:\n",
    "\n",
    "1. **Универсальность**  \n",
    "   SVD (`gelsd`) работает во всех случаях — даже если матрица $\\mathbf{X}$ вырождена, имеет мультиколлинеарность или число признаков превышает число объектов.\n",
    "\n",
    "2. **Надёжность \"из коробки\"**  \n",
    "   `scikit-learn` стремится к простоте и предсказуемости: пользователь должен получать устойчивый результат без необходимости настраивать низкоуровневые параметры.\n",
    "\n",
    "3. **Минимизация ошибок пользователя**  \n",
    "   Если бы был выбор между SVD и QR, пользователь мог бы выбрать QR на плохих данных — получить нестабильные веса или не понять, почему модель ведёт себя странно.\n",
    "\n",
    "4. **Производительность vs стабильность**  \n",
    "   QR быстрее на хорошо обусловленных данных.  \n",
    "   SVD (`gelsd`) — оптимальный компромисс между скоростью и устойчивостью для большинства реальных задач.\n",
    "\n",
    "\n",
    "### QR-разложение напрямую можно реализовать через `scipy`:\n",
    "\n",
    "```python\n",
    "from scipy.linalg import qr, solve_triangular\n",
    "\n",
    "Q, R = qr(X, mode='economic')\n",
    "w = solve_triangular(R, Q.T @ y)  # w = R^{-1} Q^T y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
